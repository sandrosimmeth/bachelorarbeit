{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8bbe369-86da-444d-a79b-ffd204f402b7",
   "metadata": {},
   "source": [
    "### XAI-Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "143279cd-b331-4105-80f6-195cf3d66ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 15:16:55.226614: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2026-01-05 15:16:55.250310: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2026-01-05 15:16:55.250339: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2026-01-05 15:16:55.266552: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-05 15:16:56.095031: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starte Daten-Selektion mit GROUP-AWARE SPLIT...\n",
      "Test-Set rekonstruiert: 1823 Bilder verfügbar.\n",
      "Filtere Test-Set nach den 5 Zielklassen (Ziel: 10 Bilder/Klasse)...\n",
      "  Prüfe Klasse: Tomato___Late_blight (Verfügbar im Testset: 27)\n",
      "  -> 10 Bilder final ausgewählt.\n",
      "  Prüfe Klasse: Tomato___Tomato_Yellow_Leaf_Curl_Virus (Verfügbar im Testset: 175)\n",
      "    -> Info: Nur 3 Bilder > 300px gefunden. Fülle 7 mit kleineren Bildern auf.\n",
      "  -> 10 Bilder final ausgewählt.\n",
      "  Prüfe Klasse: Squash___Powdery_mildew (Verfügbar im Testset: 42)\n",
      "  -> 10 Bilder final ausgewählt.\n",
      "  Prüfe Klasse: Tomato___Bacterial_spot (Verfügbar im Testset: 50)\n",
      "  -> 10 Bilder final ausgewählt.\n",
      "  Prüfe Klasse: Tomato___healthy (Verfügbar im Testset: 70)\n",
      "    -> Info: Nur 8 Bilder > 300px gefunden. Fülle 2 mit kleineren Bildern auf.\n",
      "  -> 10 Bilder final ausgewählt.\n",
      "Insgesamt ausgewählt: 50 Bilder.\n",
      "\n",
      "Starte Heatmap-Generierung...\n",
      "Lade Model_A...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 15:17:02.115677: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2026-01-05 15:17:02.203379: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2026-01-05 15:17:02.203460: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2026-01-05 15:17:02.205745: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2026-01-05 15:17:02.205809: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2026-01-05 15:17:02.205857: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2026-01-05 15:17:02.366481: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2026-01-05 15:17:02.366574: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2026-01-05 15:17:02.366588: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2026-01-05 15:17:02.366656: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2026-01-05 15:17:02.366686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9261 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:08:00.0, compute capability: 6.1\n",
      "/home/sandro/tf_env/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:797: UserWarning: Skipping variable loading for optimizer 'adamax', because it has 436 variables whereas the saved optimizer has 440 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Layer gefunden: efficientnetb0 (Shape: (None, 7, 7, 1280))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 15:17:12.751979: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "/home/sandro/tf_env/lib/python3.12/site-packages/keras/src/models/functional.py:241: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: ['input_layer']\n",
      "Received: inputs=Tensor(shape=(1, 224, 224, 3))\n",
      "  warnings.warn(msg)\n",
      "/tmp/ipykernel_523/2210275580.py:100: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  jet = cm.get_cmap(\"jet\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade Model_B...\n",
      "Target Layer gefunden: efficientnetb0 (Shape: (None, 7, 7, 1280))\n",
      "Lade Model_C...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sandro/tf_env/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:797: UserWarning: Skipping variable loading for optimizer 'adam', because it has 436 variables whereas the saved optimizer has 440 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Layer gefunden: efficientnetb0 (Shape: (None, 7, 7, 1280))\n",
      "Lade Model_D...\n",
      "Target Layer gefunden: efficientnetb0 (Shape: (None, 7, 7, 1280))\n",
      "\n",
      "FERTIG!\n",
      "Bilder in: ./blind_study\n",
      "Dateien:\n",
      "1. blind_rating_KEY_FINAL.csv (LÖSUNG)\n",
      "2. BEWERTUNGSBOGEN_FINAL.csv (ARBEITSDATEI)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pathlib\n",
    "import uuid\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split, StratifiedGroupKFold\n",
    "from tensorflow import keras\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# KONFIGURATION\n",
    "EXPERIMENT_SEED = 42\n",
    "IMG_SIZE = (224, 224)\n",
    "PLANTDOC_DIR = '/mnt/c/Users/sandr/Desktop/Studium/thesis/praxis/data/plantdoc_cropped'\n",
    "OUTPUT_DIR = \"./blind_study\"\n",
    "MODELS_DIR = \"./models\"\n",
    "\n",
    "# Die 5 Klassen für die Analyse\n",
    "TARGET_CLASSES = [\n",
    "    \"Tomato___Late_blight\",\n",
    "    \"Tomato___Tomato_Yellow_Leaf_Curl_Virus\",\n",
    "    \"Squash___Powdery_mildew\",\n",
    "    \"Tomato___Bacterial_spot\",\n",
    "    \"Tomato___healthy\"\n",
    "]\n",
    "\n",
    "IMAGES_PER_CLASS = 10\n",
    "MIN_PIXEL_SIZE = 300  # Wunschgröße (wird unterschritten, falls nötig)\n",
    "HEATMAP_ALPHA = 0.6   # Sichtbarkeit der Heatmap\n",
    "\n",
    "# Alle 4 Modelle\n",
    "model_files = {\n",
    "    \"Model_A\": f\"model_A_seed_{EXPERIMENT_SEED}.keras\",\n",
    "    \"Model_B\": f\"model_B_seed_{EXPERIMENT_SEED}.keras\",\n",
    "    \"Model_C\": f\"model_C_seed_{EXPERIMENT_SEED}.keras\",\n",
    "    \"Model_D\": f\"model_D_seed_{EXPERIMENT_SEED}.keras\"\n",
    "}\n",
    "\n",
    "# GRAD-CAM Implementierung [https://keras.io/examples/vision/grad_cam/]\n",
    "def find_target_layer(model):\n",
    "    \"\"\"Sucht den letzten Layer im Hauptmodell, der einen 4D-Output hat.\"\"\"\n",
    "    for layer in reversed(model.layers):\n",
    "        if hasattr(layer, 'output_shape'):\n",
    "            shape = layer.output_shape\n",
    "            if isinstance(shape, list):\n",
    "                shape = shape[0]\n",
    "            if len(shape) == 4 and 'input' not in layer.name.lower():\n",
    "                print(f\"Target Layer gefunden: {layer.name} (Shape: {shape})\")\n",
    "                return layer.name\n",
    "    return 'efficientnetb0'\n",
    "\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    conv_layer = model.get_layer(last_conv_layer_name)\n",
    "    \n",
    "    # Modell in Conv-Teil und Classifier-Teil splitten\n",
    "    base_model_index = -1\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        if layer.name == last_conv_layer_name:\n",
    "            base_model_index = i\n",
    "            break\n",
    "            \n",
    "    head_layers = model.layers[base_model_index + 1:]\n",
    "    \n",
    "    conv_model = keras.models.Model(inputs=conv_layer.inputs, outputs=conv_layer.output)\n",
    "    \n",
    "    head_input = keras.Input(shape=conv_layer.output_shape[1:])\n",
    "    x = head_input\n",
    "    for layer in head_layers:\n",
    "        x = layer(x)\n",
    "    head_model = keras.models.Model(inputs=head_input, outputs=x)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs = conv_model(img_array)\n",
    "        tape.watch(conv_outputs)\n",
    "        predictions = head_model(conv_outputs)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(predictions[0])\n",
    "        class_channel = predictions[:, pred_index]\n",
    "\n",
    "    grads = tape.gradient(class_channel, conv_outputs)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "    conv_outputs = conv_outputs[0]\n",
    "    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "    \n",
    "    epsilon = tf.keras.backend.epsilon()\n",
    "    heatmap = tf.maximum(heatmap, 0) / (tf.math.reduce_max(heatmap) + epsilon)\n",
    "    \n",
    "    return heatmap.numpy()\n",
    "\n",
    "def save_and_display_gradcam(img_path, heatmap, save_path, alpha=0.4):\n",
    "    # Originalbild laden\n",
    "    img = keras.preprocessing.image.load_img(img_path)\n",
    "    img = keras.preprocessing.image.img_to_array(img)\n",
    "\n",
    "    # Heatmap einfärben\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    jet = cm.get_cmap(\"jet\")\n",
    "    jet_colors = jet(np.arange(256))[:, :3]\n",
    "    jet_heatmap = jet_colors[heatmap]\n",
    "\n",
    "    # Heatmap auf Bildgröße skalieren\n",
    "    jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n",
    "    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
    "    jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n",
    "\n",
    "    # Überlagern\n",
    "    superimposed_img = jet_heatmap * alpha + img\n",
    "    superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n",
    "    superimposed_img.save(save_path)\n",
    "\n",
    "# --- HELPER: BILD-AUSWAHL MIT FALLBACK ---\n",
    "def select_images_robust(candidates, target_count, min_size):\n",
    "    \"\"\"\n",
    "    Versucht target_count Bilder >= min_size zu finden.\n",
    "    Wenn nicht genug da sind, füllt es mit den nächstbesten (kleineren) Bildern auf.\n",
    "    \"\"\"\n",
    "    valid_large = []\n",
    "    valid_small = []\n",
    "    \n",
    "    for img_path in candidates:\n",
    "        try:\n",
    "            img = keras.utils.load_img(img_path)\n",
    "            w, h = img.size\n",
    "            if w >= min_size and h >= min_size:\n",
    "                valid_large.append(img_path)\n",
    "            else:\n",
    "                # Wir merken uns auch die kleinen, inkl. Größe für Sortierung\n",
    "                valid_small.append((img_path, w*h)) \n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    # Zufallsauswahl bei den großen (um Bias zu vermeiden)\n",
    "    rng = np.random.RandomState(EXPERIMENT_SEED)\n",
    "    rng.shuffle(valid_large)\n",
    "    \n",
    "    selected = []\n",
    "    \n",
    "    # 1. Nimm so viele Große wie möglich (bis target_count)\n",
    "    take_large = min(len(valid_large), target_count)\n",
    "    selected.extend(valid_large[:take_large])\n",
    "    \n",
    "    # 2. Prüfen ob wir noch mehr brauchen\n",
    "    missing = target_count - len(selected)\n",
    "    \n",
    "    if missing > 0:\n",
    "        if len(valid_small) > 0:\n",
    "            print(f\"    -> Info: Nur {len(selected)} Bilder > {min_size}px gefunden. Fülle {missing} mit kleineren Bildern auf.\")\n",
    "            # Sortiere kleine Bilder nach Größe (Pixelanzahl), absteigend -> Die \"größten Kleinen\" zuerst\n",
    "            valid_small.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            # Die Pfade der größten kleinen Bilder holen\n",
    "            top_small_paths = [x[0] for x in valid_small[:missing]]\n",
    "            selected.extend(top_small_paths)\n",
    "        else:\n",
    "            print(f\"    -> WARNUNG: Auch mit kleinen Bildern konnten nur {len(selected)} gefunden werden!\")\n",
    "            \n",
    "    return selected\n",
    "\n",
    "# --- HAUPTPROGRAMM ---\n",
    "\n",
    "# 1. Daten laden und splitten\n",
    "print(\"Starte Daten-Selektion mit GROUP-AWARE SPLIT...\")\n",
    "data_dir = pathlib.Path(PLANTDOC_DIR)\n",
    "\n",
    "# Sortieren für deterministische Reihenfolge (WICHTIG!)\n",
    "image_paths = sorted(list(data_dir.glob('*/*')))\n",
    "image_paths = [str(path) for path in image_paths if not pathlib.Path(path).name.startswith('.')]\n",
    "\n",
    "class_names = sorted([item.name for item in data_dir.glob('*') if item.is_dir()])\n",
    "class_to_index = {name: i for i, name in enumerate(class_names)}\n",
    "\n",
    "all_labels = []\n",
    "all_groups = []\n",
    "\n",
    "for path in image_paths:\n",
    "    # Label\n",
    "    label = class_to_index[pathlib.Path(path).parent.name]\n",
    "    all_labels.append(label)\n",
    "    \n",
    "    # Group ID (für GroupKFold)\n",
    "    filename = os.path.basename(path)\n",
    "    if \"_crop_\" in filename:\n",
    "        group_id = filename.rsplit(\"_crop_\", 1)[0]\n",
    "    else:\n",
    "        group_id = filename\n",
    "    all_groups.append(group_id)\n",
    "\n",
    "# Split reproduzieren (Group-Aware!)\n",
    "# Nutzen desselben Seeds und derselben Logik wie im Training\n",
    "cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=EXPERIMENT_SEED)\n",
    "\n",
    "# Erster Fold ist das Test-Set\n",
    "_, test_idx = next(cv.split(image_paths, all_labels, all_groups))\n",
    "\n",
    "# Test-Pfade und Test-Labels\n",
    "test_paths = [image_paths[i] for i in test_idx]\n",
    "test_labels = [all_labels[i] for i in test_idx]\n",
    "\n",
    "print(f\"Test-Set rekonstruiert: {len(test_paths)} Bilder verfügbar.\")\n",
    "\n",
    "\n",
    "# 2. Bilder auswählen\n",
    "print(f\"Filtere Test-Set nach den {len(TARGET_CLASSES)} Zielklassen (Ziel: {IMAGES_PER_CLASS} Bilder/Klasse)...\")\n",
    "selected_paths = []\n",
    "\n",
    "for target_class in TARGET_CLASSES:\n",
    "    if target_class not in class_names:\n",
    "        print(f\"WARNUNG: Klasse '{target_class}' nicht im Datensatz gefunden!\")\n",
    "        continue\n",
    "        \n",
    "    class_idx = class_to_index[target_class]\n",
    "    \n",
    "    # Kandidaten finden (nur im sauberen Test-Set suchen!)\n",
    "    candidates = [test_paths[i] for i, label_idx in enumerate(test_labels) if label_idx == class_idx]\n",
    "    \n",
    "    # Robuste Auswahl aufrufen\n",
    "    print(f\"  Prüfe Klasse: {target_class} (Verfügbar im Testset: {len(candidates)})\")\n",
    "    chosen = select_images_robust(candidates, IMAGES_PER_CLASS, MIN_PIXEL_SIZE)\n",
    "    \n",
    "    selected_paths.extend(chosen)\n",
    "    print(f\"  -> {len(chosen)} Bilder final ausgewählt.\")\n",
    "\n",
    "print(f\"Insgesamt ausgewählt: {len(selected_paths)} Bilder.\")\n",
    "path_to_id = {path: i+1 for i, path in enumerate(selected_paths)}\n",
    "\n",
    "# 3. Ordner vorbereiten\n",
    "if os.path.exists(OUTPUT_DIR):\n",
    "    shutil.rmtree(OUTPUT_DIR)\n",
    "os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "blind_data = []\n",
    "\n",
    "def get_img_array(img_path, size):\n",
    "    img = keras.preprocessing.image.load_img(img_path, target_size=size)\n",
    "    array = keras.preprocessing.image.img_to_array(img)\n",
    "    array = np.expand_dims(array, axis=0)\n",
    "    return array\n",
    "\n",
    "# 4. Heatmaps generieren\n",
    "print(\"\\nStarte Heatmap-Generierung...\")\n",
    "\n",
    "for model_key, filename in model_files.items():\n",
    "    full_model_path = os.path.join(MODELS_DIR, filename)\n",
    "    if not os.path.exists(full_model_path):\n",
    "        print(f\"WARNUNG: {filename} fehlt.\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"Lade {model_key}...\")\n",
    "    model = keras.models.load_model(full_model_path)\n",
    "    last_conv_layer = find_target_layer(model)\n",
    "            \n",
    "    for img_path in selected_paths:\n",
    "        img_id = path_to_id[img_path]\n",
    "        true_label_name = pathlib.Path(img_path).parent.name\n",
    "        \n",
    "        try:\n",
    "            img_array = get_img_array(img_path, IMG_SIZE)\n",
    "            \n",
    "            # Vorhersage holen\n",
    "            preds = model.predict(img_array, verbose=0)\n",
    "            pred_idx = np.argmax(preds[0])\n",
    "            pred_label_name = class_names[pred_idx]\n",
    "            confidence = np.max(preds[0])\n",
    "            \n",
    "            # PREDICTED vs TRUE CLASS\n",
    "            target_class_index = pred_idx\n",
    "            \n",
    "            # Ausnahme: Bacterial Spot (Die Phantom-Klasse) -> Force Ground Truth\n",
    "            if true_label_name == \"Tomato___Bacterial_spot\":\n",
    "                 target_class_index = class_to_index[true_label_name]\n",
    "            \n",
    "            # Heatmap berechnen\n",
    "            heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer, pred_index=target_class_index)\n",
    "            \n",
    "            # Speichern (Hash-Name)\n",
    "            random_suffix = str(uuid.uuid4())[:8]\n",
    "            unique_filename = f\"{img_id:03d}_{random_suffix}.jpg\"\n",
    "            save_path = os.path.join(OUTPUT_DIR, unique_filename)\n",
    "            \n",
    "            save_and_display_gradcam(img_path, heatmap, save_path, alpha=HEATMAP_ALPHA)\n",
    "\n",
    "            blind_data.append({\n",
    "                \"Rating_ID (Filename)\": unique_filename,\n",
    "                \"Image_ID\": img_id,\n",
    "                \"Model (Hidden)\": model_key, \n",
    "                \"True_Class\": true_label_name,\n",
    "                \"Pred_Class\": pred_label_name, # Wird nur im Key gespeichert\n",
    "                \"Target_Class_Used_For_Heatmap\": class_names[target_class_index],\n",
    "                \"Correct\": (true_label_name == pred_label_name),\n",
    "                \"Confidence\": confidence\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler bei Bild {img_id}: {e}\")\n",
    "\n",
    "# 5. Export\n",
    "if blind_data:\n",
    "    df_full = pd.DataFrame(blind_data)\n",
    "    df_full = df_full.sort_values(by=[\"Image_ID\", \"Rating_ID (Filename)\"])\n",
    "\n",
    "    # KEY FILE (Lösung)\n",
    "    df_full.to_csv(\"blind_rating_KEY_FINAL.csv\", index=False)\n",
    "\n",
    "    # BEWERTUNGSBOGEN (Arbeitsdatei)\n",
    "    df_rating = df_full[[\"Rating_ID (Filename)\", \"Image_ID\", \"True_Class\"]].copy() \n",
    "    df_rating[\"Rating (1=Noise, 3=Mix, 5=Precise)\"] = \"\"\n",
    "    \n",
    "    df_rating.to_csv(\"BEWERTUNGSBOGEN_FINAL.csv\", index=False)\n",
    "\n",
    "    print(\"\\nFERTIG!\")\n",
    "    print(f\"Bilder in: {OUTPUT_DIR}\")\n",
    "    print(\"Dateien:\")\n",
    "    print(\"1. blind_rating_KEY_FINAL.csv (LÖSUNG)\")\n",
    "    print(\"2. BEWERTUNGSBOGEN_FINAL.csv (ARBEITSDATEI)\")\n",
    "else:\n",
    "    print(\"Keine Daten generiert.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
